{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scipy in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (1.15.2)\n",
      "Requirement already satisfied: torch in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchaudio in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: librosa in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: pydub in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: matplotlib in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: filelock in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pycparser in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\esi\\2cs\\fas\\voix\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scipy torch torchaudio librosa pydub matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original Transcription: THE SUN ALSO RISES POWERFUL INTENSE VISUALLY MAGNIFICENT THE SUN ALSO RISES IS THE NOVEL WHICH ESTABLISHED INATEMIGWAY AS A WRITER OF GENIUS\n",
      "üîÑ Iteration 1\n",
      "üîç Loss: 0.45314472913742065\n",
      "üîÑ Iteration 2\n",
      "üîç Loss: 0.9177964329719543\n",
      "üîÑ Iteration 3\n",
      "üîç Loss: 1.4054068326950073\n",
      "üîÑ Iteration 4\n",
      "üîç Loss: 1.3769713640213013\n",
      "üîÑ Iteration 5\n",
      "üîç Loss: 1.9479230642318726\n",
      "üîÑ Iteration 6\n",
      "üîç Loss: 1.5851980447769165\n",
      "üö® Transcription after attack: THE SUN ALSO ARISES POWERFUL AND TEMTH VISUALLY MAGNIFICENT THE SUN ALSO ARISES AS THE MOLVELE WHICH ESTABLISHS ERNEST HENLINK WITH OF THE RISER OFF GENIUS\n",
      "‚úÖ Adversarial audio saved as 'TpFasi_adversarial_PGD.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "def convert_audio(input_path):\n",
    "    base, ext = os.path.splitext(input_path)\n",
    "    output_path = f\"{base}_converted.wav\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True)\n",
    "\n",
    "AUDIO_PATH = \"Tsg_long_version.wav\"\n",
    "AUDIO_PATH = convert_audio(AUDIO_PATH)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    raise ValueError(f\"Incorrect sample rate: {sample_rate} Hz\")\n",
    "\n",
    "waveform = waveform.mean(dim=0)\n",
    "waveform = waveform / waveform.abs().max()\n",
    "\n",
    "inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(1)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_original = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"‚úÖ Original Transcription: {text_original}\")\n",
    "\n",
    "audio_tensor = waveform.clone().detach().requires_grad_(True)\n",
    "\n",
    "epsilon = 0.005\n",
    "alpha = 0.001\n",
    "num_iter = 6\n",
    "\n",
    "adv_audio = audio_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "target = processor.tokenizer(text_original, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "input_lengths = torch.tensor([logits.shape[1]], dtype=torch.long)\n",
    "target_lengths = torch.tensor([target.shape[0]], dtype=torch.long)\n",
    "ctc_loss = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    print(f\"üîÑ Iteration {i+1}\")\n",
    "    adv_audio = adv_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "    if adv_audio.grad is not None:\n",
    "        adv_audio.grad.zero_()\n",
    "\n",
    "    logits = model(adv_audio.unsqueeze(0)).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    loss = ctc_loss(log_probs.transpose(0, 1), target, input_lengths, target_lengths)\n",
    "    \n",
    "    loss.backward()\n",
    "    print(f\"üîç Loss: {loss.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        perturbation = alpha * torch.sign(adv_audio.grad)\n",
    "        adv_audio = adv_audio + perturbation\n",
    "        perturbation = torch.clamp(adv_audio - audio_tensor, min=-epsilon, max=epsilon)\n",
    "        adv_audio = audio_tensor + perturbation\n",
    "        adv_audio = torch.clamp(adv_audio, min=-1, max=1)\n",
    "    \n",
    "    adv_audio.requires_grad_()\n",
    "\n",
    "adversarial_audio = (adv_audio.detach().numpy() * 32768).astype(np.int16)\n",
    "wav.write(\"TpFasi_adversarial_PGD.wav\", 16000, adversarial_audio)\n",
    "\n",
    "inputs = processor(adv_audio.unsqueeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(0)  # Suppression correcte de la premi√®re dimension\n",
    "inputs.input_values = inputs.input_values.squeeze().unsqueeze(0)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_adversarial = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"üö® Transcription after attack: {text_adversarial}\")\n",
    "print(\"‚úÖ Adversarial audio saved as 'TpFasi_adversarial_PGD.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def bark_scale(freq):\n",
    "    \"\"\"Convert frequency to the Bark scale.\"\"\"\n",
    "    return 13 * np.arctan(0.00076 * freq) + 3.5 * np.arctan((freq / 7500.0) ** 2)\n",
    "\n",
    "def apply_psychoacoustic_masking(waveform, sr=16000):\n",
    "    \"\"\"Apply a psychoacoustic masking technique to hide perturbations.\"\"\"\n",
    "    waveform_np = waveform.detach().cpu().numpy()  # D√©tacher le tenseur avant conversion\n",
    "    stft = librosa.stft(waveform_np, n_fft=512, hop_length=128)\n",
    "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "\n",
    "    # Get frequency bins\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)\n",
    "    bark_bins = bark_scale(freqs)\n",
    "\n",
    "    # Apply masking: reduce noise on perceptually sensitive frequencies\n",
    "    mask = np.exp(-bark_bins / np.max(bark_bins))  # Higher Bark ‚Üí lower modification\n",
    "    masked_magnitude = magnitude * mask[:, np.newaxis]\n",
    "\n",
    "    # Reconstruct signal\n",
    "    modified_stft = masked_magnitude * np.exp(1j * phase)\n",
    "    modified_waveform = librosa.istft(modified_stft, hop_length=128)\n",
    "\n",
    "    return torch.tensor(modified_waveform, dtype=torch.float32)\n",
    "\n",
    "# Appliquer la correction et tester\n",
    "adv_audio = apply_psychoacoustic_masking(adv_audio)\n",
    "\n",
    "\n",
    "# Apply psychoacoustic filtering to perturbation\n",
    "adv_audio = apply_psychoacoustic_masking(adv_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original Transcription: THE SUN ALSO RISES POWERFUL INTENSE VISUALLY MAGNIFICENT THE SUN ALSO RISES IS THE NOVEL WHICH ESTABLISHED INATEMIGWAY AS A WRITER OF GENIUS\n",
      "üîÑ Iteration 1\n",
      "üîç Loss: 0.45314472913742065\n",
      "üîÑ Iteration 2\n",
      "üîç Loss: 1.0092693567276\n",
      "üîÑ Iteration 3\n",
      "üîç Loss: 1.4539203643798828\n",
      "üîÑ Iteration 4\n",
      "üîç Loss: 1.3525958061218262\n",
      "üîÑ Iteration 5\n",
      "üîç Loss: 1.8819913864135742\n",
      "üö® Transcription after attack (psychoacoustic applied): THE SUN ALSO ARISES POWERFUL AND PREVISUALLY MAGNIFICENT THE SUN ALSO ARIVES IS A NOVEL WHICH ESTABLISHED EARNEST HIMIG WITH OTHER WRITER OF GENIUS\n",
      "‚úÖ Adversarial audio saved as 'TpFasi_adversarial_PGD_psychoacoustic.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "def convert_audio(input_path):\n",
    "    base, ext = os.path.splitext(input_path)\n",
    "    output_path = f\"{base}_converted.wav\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "def bark_scale(freq):\n",
    "    \"\"\"Convert frequency to the Bark scale.\"\"\"\n",
    "    return 13 * np.arctan(0.00076 * freq) + 3.5 * np.arctan((freq / 7500.0) ** 2)\n",
    "\n",
    "def apply_psychoacoustic_masking(waveform, sr=16000):\n",
    "    \"\"\"Appliquer un masquage psychoacoustique pour cacher les perturbations.\"\"\"\n",
    "    waveform_np = waveform.detach().cpu().numpy()  # D√©tacher pour utiliser librosa\n",
    "    stft = librosa.stft(waveform_np, n_fft=512, hop_length=128)\n",
    "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "\n",
    "    # Appliquer une pond√©ration bas√©e sur l'√©chelle de Bark\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)\n",
    "    bark_bins = bark_scale(freqs)\n",
    "    mask = np.exp(-bark_bins / np.max(bark_bins))  # Plus Bark est haut, moins de modification\n",
    "    masked_magnitude = magnitude * mask[:, np.newaxis]\n",
    "\n",
    "    # Reconstruction du signal\n",
    "    modified_stft = masked_magnitude * np.exp(1j * phase)\n",
    "    modified_waveform = librosa.istft(modified_stft, hop_length=128)\n",
    "\n",
    "    return torch.tensor(modified_waveform, dtype=torch.float32)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True)\n",
    "\n",
    "AUDIO_PATH = \"Tsg_long_version.wav\"\n",
    "AUDIO_PATH = convert_audio(AUDIO_PATH)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    raise ValueError(f\"Incorrect sample rate: {sample_rate} Hz\")\n",
    "\n",
    "waveform = waveform.mean(dim=0)\n",
    "waveform = waveform / waveform.abs().max()\n",
    "\n",
    "inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(1)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_original = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"‚úÖ Original Transcription: {text_original}\")\n",
    "\n",
    "audio_tensor = waveform.clone().detach().requires_grad_(True)\n",
    "\n",
    "epsilon = 0.015\n",
    "alpha = epsilon / 3\n",
    "num_iter = min(10,int(epsilon/alpha)+2)\n",
    "\n",
    "adv_audio = audio_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "target = processor.tokenizer(text_original, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "input_lengths = torch.tensor([logits.shape[1]], dtype=torch.long)\n",
    "target_lengths = torch.tensor([target.shape[0]], dtype=torch.long)\n",
    "ctc_loss = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    print(f\"üîÑ Iteration {i+1}\")\n",
    "    adv_audio = adv_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "    if adv_audio.grad is not None:\n",
    "        adv_audio.grad.zero_()\n",
    "\n",
    "    logits = model(adv_audio.unsqueeze(0)).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    loss = ctc_loss(log_probs.transpose(0, 1), target, input_lengths, target_lengths)\n",
    "    \n",
    "    loss.backward()\n",
    "    print(f\"üîç Loss: {loss.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        perturbation = alpha * torch.sign(adv_audio.grad)\n",
    "        adv_audio = adv_audio + perturbation\n",
    "        perturbation = torch.clamp(adv_audio - audio_tensor, min=-epsilon, max=epsilon)\n",
    "        adv_audio = audio_tensor + perturbation\n",
    "        adv_audio = torch.clamp(adv_audio, min=-1, max=1)\n",
    "    \n",
    "    # ‚úÖ Appliquer le masquage psychoacoustique apr√®s chaque mise √† jour\n",
    "    adv_audio = apply_psychoacoustic_masking(adv_audio)\n",
    "    if adv_audio.shape[0] < audio_tensor.shape[0]:\n",
    "    # Zero-padding si la sortie de istft est trop courte\n",
    "        adv_audio = torch.nn.functional.pad(adv_audio, (0, audio_tensor.shape[0] - adv_audio.shape[0]))\n",
    "    elif adv_audio.shape[0] > audio_tensor.shape[0]:\n",
    "    # Tronquer si la sortie de istft est trop longue\n",
    "        adv_audio = adv_audio[:audio_tensor.shape[0]]\n",
    "    adv_audio.requires_grad_()\n",
    "\n",
    "# Sauvegarde de l'audio apr√®s application du masquage psychoacoustique\n",
    "adversarial_audio = (adv_audio.detach().numpy() * 32768).astype(np.int16)\n",
    "wav.write(\"TpFasi_adversarial_PGD_psychoacoustic.wav\", 16000, adversarial_audio)\n",
    "\n",
    "# V√©rifier la transcription apr√®s attaque\n",
    "inputs = processor(adv_audio.unsqueeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(0)\n",
    "inputs.input_values = inputs.input_values.squeeze().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_adversarial = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"üö® Transcription after attack (psychoacoustic applied): {text_adversarial}\")\n",
    "print(\"‚úÖ Adversarial audio saved as 'TpFasi_adversarial_PGD_psychoacoustic.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original Transcription: THE SUN ALSO RISES POWERFUL INTENSE VISUALLY MAGNIFICENT THE SUN ALSO RISES IS THE NOVEL WHICH ESTABLISHED INATEMIGWAY AS A WRITER OF GENIUS\n",
      "üîç Perturbation Norm: 18.617125\n",
      "üîÑ Iteration 1\n",
      "üîç Loss: 13.497586250305176\n",
      "üîÑ Iteration 2\n",
      "üîç Loss: 12.681798934936523\n",
      "üîÑ Iteration 3\n",
      "üîç Loss: 12.561518669128418\n",
      "üîÑ Iteration 4\n",
      "üîç Loss: 12.978765487670898\n",
      "üîÑ Iteration 5\n",
      "üîç Loss: 16.039358139038086\n",
      "üîÑ Iteration 6\n",
      "üîç Loss: 15.965095520019531\n",
      "üîÑ Iteration 7\n",
      "üîç Loss: 19.217084884643555\n",
      "üîÑ Iteration 8\n",
      "üîç Loss: 13.925923347473145\n",
      "üîÑ Iteration 9\n",
      "üîç Loss: 17.148847579956055\n",
      "üîÑ Iteration 10\n",
      "üîç Loss: 14.865665435791016\n",
      "üö® Transcription after attack (psychoacoustic applied): THE SUN ALSO ARISES POWERFUL AND TENSS VISUALLY MAGNIFICENT THE SUN ALSO ARISES IS THE NOVEL WHICH ESTABLISHED AN NERTAINMENT WITH THE WRITER OF GENIUS\n",
      "üîç Max absolute perturbation: 0.175500\n",
      "‚úÖ Adversarial audio saved as 'TpFasi_adversarial_PGD_psychoacoustic.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "def convert_audio(input_path):\n",
    "    base, ext = os.path.splitext(input_path)\n",
    "    output_path = f\"{base}_converted.wav\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "def bark_scale(freq):\n",
    "    \"\"\"Convert frequency to the Bark scale.\"\"\"\n",
    "    return 13 * np.arctan(0.00076 * freq) + 3.5 * np.arctan((freq / 7500.0) ** 2)\n",
    "\n",
    "def apply_psychoacoustic_masking(waveform, sr=16000):\n",
    "    \"\"\"Appliquer un masquage psychoacoustique pour cacher les perturbations.\"\"\"\n",
    "    waveform_np = waveform.detach().cpu().numpy()  # D√©tacher pour utiliser librosa\n",
    "    stft = librosa.stft(waveform_np, n_fft=512, hop_length=128)\n",
    "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "\n",
    "    # Appliquer une pond√©ration bas√©e sur l'√©chelle de Bark\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)\n",
    "    bark_bins = bark_scale(freqs)\n",
    "    mask = np.exp(-bark_bins / np.max(bark_bins))  # Plus Bark est haut, moins de modification\n",
    "    masked_magnitude = magnitude * mask[:, np.newaxis]\n",
    "\n",
    "    # Reconstruction du signal\n",
    "    modified_stft = masked_magnitude * np.exp(1j * phase)\n",
    "    modified_waveform = librosa.istft(modified_stft, hop_length=128)\n",
    "\n",
    "    return torch.tensor(modified_waveform, dtype=torch.float32)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True)\n",
    "\n",
    "AUDIO_PATH = \"Tsg_long_version.wav\"\n",
    "AUDIO_PATH = convert_audio(AUDIO_PATH)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    raise ValueError(f\"Incorrect sample rate: {sample_rate} Hz\")\n",
    "\n",
    "waveform = waveform.mean(dim=0)\n",
    "waveform = waveform / waveform.abs().max()\n",
    "\n",
    "inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(1)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_original = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"‚úÖ Original Transcription: {text_original}\")\n",
    "\n",
    "audio_tensor = waveform.clone().detach().requires_grad_(True)\n",
    "\n",
    "epsilon = 0.03\n",
    "alpha = epsilon / 3.5\n",
    "num_iter = 10\n",
    "perturbation_norm = torch.norm(adv_audio - audio_tensor).item()\n",
    "print(f\"üîç Perturbation Norm: {perturbation_norm:.6f}\")\n",
    "\n",
    "adv_audio = audio_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "target = processor.tokenizer(text_original, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "input_lengths = torch.tensor([logits.shape[1]], dtype=torch.long)\n",
    "target_lengths = torch.tensor([target.shape[0]], dtype=torch.long)\n",
    "ctc_loss = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    print(f\"üîÑ Iteration {i+1}\")\n",
    "    adv_audio = adv_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "    if adv_audio.grad is not None:\n",
    "        adv_audio.grad.zero_()\n",
    "\n",
    "    logits = model(adv_audio.unsqueeze(0)).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    entropy_loss = -torch.sum(log_probs * torch.exp(log_probs))  # Increase uncertainty\n",
    "    alignment_loss = ctc_loss(log_probs.transpose(0, 1), target, input_lengths, target_lengths)\n",
    "    loss = alignment_loss + 0.25 * entropy_loss  # Weighted attack\n",
    "\n",
    "    loss.backward()\n",
    "    print(f\"üîç Loss: {loss.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        adaptive_alpha = alpha * torch.abs(adv_audio.grad) / torch.max(torch.abs(adv_audio.grad))\n",
    "        perturbation = adaptive_alpha * torch.sign(adv_audio.grad)\n",
    "        adv_audio = adv_audio + perturbation\n",
    "        perturbation = torch.clamp(adv_audio - audio_tensor, min=-epsilon, max=epsilon)\n",
    "        adv_audio = audio_tensor + perturbation\n",
    "        adv_audio = torch.clamp(adv_audio, min=-1, max=1)\n",
    "    \n",
    "    energy = torch.abs(torch.stft(adv_audio, n_fft=512, return_complex=True)).mean(dim=1)\n",
    "    high_energy_mask = (energy > torch.mean(energy)).float()\n",
    "\n",
    "    # Upsample high_energy_mask to match adv_audio's length\n",
    "    high_energy_mask = torch.nn.functional.interpolate(\n",
    "        high_energy_mask.unsqueeze(0).unsqueeze(0),  # Add batch & channel dims\n",
    "        size=adv_audio.shape[0],  # Resize to match waveform length\n",
    "        mode=\"linear\",  # Use linear interpolation for smooth scaling\n",
    "        align_corners=False\n",
    "    ).squeeze()  # Remove extra dims\n",
    "\n",
    "    energy_factor = 0.85  # Control how much energy is preserved (0.0 = mute, 1.0 = no change)\n",
    "    adaptive_mask = energy_factor + (1 - energy_factor) * high_energy_mask\n",
    "    adv_audio = adv_audio * adaptive_mask\n",
    "\n",
    "    adv_audio.requires_grad_()\n",
    "\n",
    "# Sauvegarde de l'audio apr√®s application du masquage psychoacoustique\n",
    "adversarial_audio = (adv_audio.detach().numpy() * 32768).astype(np.int16)\n",
    "wav.write(\"TpFasi_adversarial_PGD_psychoacoustic.wav\", 16000, adversarial_audio)\n",
    "\n",
    "# V√©rifier la transcription apr√®s attaque\n",
    "inputs = processor(adv_audio.unsqueeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(0)\n",
    "inputs.input_values = inputs.input_values.squeeze().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_adversarial = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"üö® Transcription after attack (psychoacoustic applied): {text_adversarial}\")\n",
    "print(f\"üîç Max absolute perturbation: {torch.max(torch.abs(adv_audio - audio_tensor)).item():.6f}\")\n",
    "print(\"‚úÖ Adversarial audio saved as 'TpFasi_adversarial_PGD_psychoacoustic.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original Transcription: VOICES AND ACTIVISTS HAVE BEEN ROUTINELY DENOUNCED MISREPRESENTED AND TARGETED BY MANY NATIONAL MEDIA OUTLETS THE RIGHT WING MEDIA HAS BEEN PARTICULARLY HOSTILE\n",
      "üîç Perturbation Norm: 0.000000\n",
      "üîÑ Iteration 1\n",
      "üîç Loss: 7.158161163330078\n",
      "üîÑ Iteration 2\n",
      "üîç Loss: 8.392208099365234\n",
      "üîÑ Iteration 3\n",
      "üîç Loss: 8.997980117797852\n",
      "üîÑ Iteration 4\n",
      "üîç Loss: 9.482826232910156\n",
      "üîÑ Iteration 5\n",
      "üîç Loss: 10.967926025390625\n",
      "üîÑ Iteration 6\n",
      "üîç Loss: 12.678486824035645\n",
      "üîÑ Iteration 7\n",
      "üîç Loss: 16.21771240234375\n",
      "üîÑ Iteration 8\n",
      "üîç Loss: 14.609498023986816\n",
      "üîÑ Iteration 9\n",
      "üîç Loss: 17.93812370300293\n",
      "üîÑ Iteration 10\n",
      "üîç Loss: 18.32475471496582\n",
      "üö® Transcription after attack (psychoacoustic applied): VOICE WAS AND ACTIVIUS TO HAVE BEEN WITH KEENLY GENAU MITE PRESENTED AND TAGGETED BY MANY NATRIMAL MEDIA OUTLET THE RIGHT WINGED MEDIA HAD BEEN PARTICULARLY ALL TORA\n",
      "üîç Max absolute perturbation: 0.321000\n",
      "‚úÖ Adversarial audio saved as 'Test_tp_fasii_PGD_attack.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "def convert_audio(input_path):\n",
    "    base, ext = os.path.splitext(input_path)\n",
    "    output_path = f\"{base}_converted.wav\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "def bark_scale(freq):\n",
    "    \"\"\"Convert frequency to the Bark scale.\"\"\"\n",
    "    return 13 * np.arctan(0.00076 * freq) + 3.5 * np.arctan((freq / 7500.0) ** 2)\n",
    "\n",
    "def apply_psychoacoustic_masking(waveform, sr=16000):\n",
    "    \"\"\"Appliquer un masquage psychoacoustique pour cacher les perturbations.\"\"\"\n",
    "    waveform_np = waveform.detach().cpu().numpy()  # D√©tacher pour utiliser librosa\n",
    "    stft = librosa.stft(waveform_np, n_fft=512, hop_length=128)\n",
    "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "\n",
    "    # Appliquer une pond√©ration bas√©e sur l'√©chelle de Bark\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)\n",
    "    bark_bins = bark_scale(freqs)\n",
    "    mask = np.exp(-bark_bins / np.max(bark_bins))  # Plus Bark est haut, moins de modification\n",
    "    masked_magnitude = magnitude * mask[:, np.newaxis]\n",
    "\n",
    "    # Reconstruction du signal\n",
    "    modified_stft = masked_magnitude * np.exp(1j * phase)\n",
    "    modified_waveform = librosa.istft(modified_stft, hop_length=128)\n",
    "\n",
    "    return torch.tensor(modified_waveform, dtype=torch.float32)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True)\n",
    "\n",
    "AUDIO_PATH = \"Test_tp_fasii.wav\"\n",
    "AUDIO_PATH = convert_audio(AUDIO_PATH)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    raise ValueError(f\"Incorrect sample rate: {sample_rate} Hz\")\n",
    "\n",
    "waveform = waveform.mean(dim=0)\n",
    "waveform = waveform / waveform.abs().max()\n",
    "\n",
    "inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(1)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_original = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"‚úÖ Original Transcription: {text_original}\")\n",
    "\n",
    "audio_tensor = waveform.clone().detach().requires_grad_(True)\n",
    "\n",
    "epsilon = 0.03\n",
    "alpha = epsilon / 4\n",
    "num_iter = 10\n",
    "adv_audio = audio_tensor.clone().detach().requires_grad_(True)\n",
    "min_length = min(audio_tensor.shape[0], adv_audio.shape[0])\n",
    "audio_tensor = audio_tensor[:min_length]\n",
    "adv_audio = adv_audio[:min_length]\n",
    "\n",
    "perturbation_norm = torch.norm(adv_audio - audio_tensor).item()\n",
    "\n",
    "print(f\"üîç Perturbation Norm: {perturbation_norm:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "target = processor.tokenizer(text_original, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "input_lengths = torch.tensor([logits.shape[1]], dtype=torch.long, device=logits.device)\n",
    "target_lengths = torch.tensor([min(target.shape[0], logits.shape[1])], dtype=torch.long, device=logits.device)\n",
    "\n",
    "ctc_loss = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    print(f\"üîÑ Iteration {i+1}\")\n",
    "    adv_audio = adv_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "    if adv_audio.grad is not None:\n",
    "        adv_audio.grad.zero_()\n",
    "\n",
    "    logits = model(adv_audio.unsqueeze(0)).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    entropy_loss = -torch.sum(log_probs * torch.exp(log_probs))  # Increase uncertainty\n",
    "    alignment_loss = ctc_loss(log_probs.transpose(0, 1), target, input_lengths, target_lengths)\n",
    "    loss = alignment_loss + 0.2 * entropy_loss  # Weighted attack\n",
    "\n",
    "    loss.backward()\n",
    "    print(f\"üîç Loss: {loss.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        adaptive_alpha = alpha * torch.abs(adv_audio.grad) / torch.max(torch.abs(adv_audio.grad))\n",
    "        perturbation = adaptive_alpha * torch.sign(adv_audio.grad)\n",
    "        adv_audio = adv_audio + perturbation\n",
    "        perturbation = torch.clamp(adv_audio - audio_tensor, min=-epsilon, max=epsilon)\n",
    "        adv_audio = audio_tensor + perturbation\n",
    "        adv_audio = torch.clamp(adv_audio, min=-1, max=1)\n",
    "    \n",
    "    energy = torch.abs(torch.stft(adv_audio, n_fft=512, return_complex=True)).mean(dim=1)\n",
    "    high_energy_mask = (energy > torch.mean(energy)).float()\n",
    "\n",
    "    # Upsample high_energy_mask to match adv_audio's length\n",
    "    high_energy_mask = torch.nn.functional.interpolate(\n",
    "        high_energy_mask.unsqueeze(0).unsqueeze(0),  # Add batch & channel dims\n",
    "        size=adv_audio.shape[0],  # Resize to match waveform length\n",
    "        mode=\"linear\",  # Use linear interpolation for smooth scaling\n",
    "        align_corners=False\n",
    "    ).squeeze()  # Remove extra dims\n",
    "\n",
    "    energy_factor = 0.7  # Control how much energy is preserved (0.0 = mute, 1.0 = no change)\n",
    "    adaptive_mask = energy_factor + (1 - energy_factor) * high_energy_mask\n",
    "    adv_audio = adv_audio * adaptive_mask\n",
    "\n",
    "    adv_audio.requires_grad_()\n",
    "\n",
    "# Sauvegarde de l'audio apr√®s application du masquage psychoacoustique\n",
    "adversarial_audio = (adv_audio.detach().numpy() * 32768).astype(np.int16)\n",
    "wav.write(\"Test_tp_fasii_PGD_attack.wav\", 16000, adversarial_audio)\n",
    "\n",
    "# V√©rifier la transcription apr√®s attaque\n",
    "inputs = processor(adv_audio.unsqueeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(0)\n",
    "inputs.input_values = inputs.input_values.squeeze().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_adversarial = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"üö® Transcription after attack (psychoacoustic applied): {text_adversarial}\")\n",
    "print(f\"üîç Max absolute perturbation: {torch.max(torch.abs(adv_audio - audio_tensor)).item():.6f}\")\n",
    "print(\"‚úÖ Adversarial audio saved as 'Test_tp_fasii_PGD_attack.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original Transcription: VOICES AND ACTIVISTS HAVE BEEN ROUTINELY DENOUNCED MISREPRESENTED AND TARGETED BY MANY NATIONAL MEDIA OUTLETS THE RIGHT WING MEDIA HAS BEEN PARTICULARLY HOSTILE\n",
      "üîç Perturbation Norm: 0.000000\n",
      "üîÑ Iteration 1\n",
      "üîç Loss: 7.158161163330078\n",
      "üîÑ Iteration 2\n",
      "üîç Loss: 7.963949203491211\n",
      "üîÑ Iteration 3\n",
      "üîç Loss: 8.918731689453125\n",
      "üîÑ Iteration 4\n",
      "üîç Loss: 9.861395835876465\n",
      "üîÑ Iteration 5\n",
      "üîç Loss: 15.046651840209961\n",
      "üîÑ Iteration 6\n",
      "üîç Loss: 14.71931266784668\n",
      "üîÑ Iteration 7\n",
      "üîç Loss: 25.932523727416992\n",
      "üîÑ Iteration 8\n",
      "üîç Loss: 21.44748306274414\n",
      "üîÑ Iteration 9\n",
      "üîç Loss: 26.30585289001465\n",
      "üîÑ Iteration 10\n",
      "üîç Loss: 28.673524856567383\n",
      "üö® Transcription after attack (psychoacoustic applied): POYFED AN PATPEVIT HAVE BEEN TRO KEENLY BENOTED MICER PRESENTED AND PARGETED BY MANY NATIMAL ME BEA OUTLET THE RIGHT WIND MAY BE A HAD BEN PARTICULARLY HALSTALL\n",
      "üîç Max absolute perturbation: 0.244678\n",
      "‚úÖ Adversarial audio saved as 'Test_tp_fasii_PGD_attack.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "def convert_audio(input_path):\n",
    "    base, ext = os.path.splitext(input_path)\n",
    "    output_path = f\"{base}_converted.wav\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "def bark_scale(freq):\n",
    "    \"\"\"Convert frequency to the Bark scale.\"\"\"\n",
    "    return 13 * np.arctan(0.00076 * freq) + 3.5 * np.arctan((freq / 7500.0) ** 2)\n",
    "\n",
    "def apply_psychoacoustic_masking(waveform, sr=16000):\n",
    "    \"\"\"Appliquer un masquage psychoacoustique pour cacher les perturbations.\"\"\"\n",
    "    waveform_np = waveform.detach().cpu().numpy()  # D√©tacher pour utiliser librosa\n",
    "    stft = librosa.stft(waveform_np, n_fft=512, hop_length=128)\n",
    "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "\n",
    "    # Appliquer une pond√©ration bas√©e sur l'√©chelle de Bark\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)\n",
    "    bark_bins = bark_scale(freqs)\n",
    "    mask = np.exp(-bark_bins / np.max(bark_bins))  # Plus Bark est haut, moins de modification\n",
    "    masked_magnitude = magnitude * mask[:, np.newaxis]\n",
    "\n",
    "    # Reconstruction du signal\n",
    "    modified_stft = masked_magnitude * np.exp(1j * phase)\n",
    "    modified_waveform = librosa.istft(modified_stft, hop_length=128)\n",
    "\n",
    "    return torch.tensor(modified_waveform, dtype=torch.float32)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True)\n",
    "\n",
    "AUDIO_PATH = \"Test_tp_fasii.wav\"\n",
    "AUDIO_PATH = convert_audio(AUDIO_PATH)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    raise ValueError(f\"Incorrect sample rate: {sample_rate} Hz\")\n",
    "\n",
    "waveform = waveform.mean(dim=0)\n",
    "waveform = waveform / waveform.abs().max()\n",
    "\n",
    "inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(1)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_original = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"‚úÖ Original Transcription: {text_original}\")\n",
    "\n",
    "audio_tensor = waveform.clone().detach().requires_grad_(True)\n",
    "\n",
    "epsilon = 0.03\n",
    "alpha = epsilon / 4\n",
    "num_iter = 10\n",
    "adv_audio = audio_tensor.clone().detach().requires_grad_(True)\n",
    "min_length = min(audio_tensor.shape[0], adv_audio.shape[0])\n",
    "audio_tensor = audio_tensor[:min_length]\n",
    "adv_audio = adv_audio[:min_length]\n",
    "\n",
    "perturbation_norm = torch.norm(adv_audio - audio_tensor).item()\n",
    "\n",
    "print(f\"üîç Perturbation Norm: {perturbation_norm:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "target = processor.tokenizer(text_original, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "input_lengths = torch.tensor([logits.shape[1]], dtype=torch.long, device=logits.device)\n",
    "target_lengths = torch.tensor([min(target.shape[0], logits.shape[1])], dtype=torch.long, device=logits.device)\n",
    "\n",
    "ctc_loss = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    print(f\"üîÑ Iteration {i+1}\")\n",
    "    adv_audio = adv_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "    if adv_audio.grad is not None:\n",
    "        adv_audio.grad.zero_()\n",
    "\n",
    "    logits = model(adv_audio.unsqueeze(0)).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    entropy_loss = -torch.sum(log_probs * torch.exp(log_probs))  # Increase uncertainty\n",
    "    alignment_loss = ctc_loss(log_probs.transpose(0, 1), target, input_lengths, target_lengths)\n",
    "    loss = alignment_loss + 0.2 * entropy_loss  # Weighted attack\n",
    "\n",
    "    loss.backward()\n",
    "    print(f\"üîç Loss: {loss.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        adaptive_alpha = alpha * torch.abs(adv_audio.grad) / torch.max(torch.abs(adv_audio.grad))\n",
    "        perturbation = adaptive_alpha * torch.sign(adv_audio.grad)\n",
    "        adv_audio = adv_audio + perturbation\n",
    "        perturbation = torch.clamp(adv_audio - audio_tensor, min=-epsilon, max=epsilon)\n",
    "        adv_audio = audio_tensor + perturbation\n",
    "        adv_audio = torch.clamp(adv_audio, min=-1, max=1)\n",
    "\n",
    "# Apply psychoacoustic masking BEFORE saving the audio\n",
    "    adv_audio = apply_psychoacoustic_masking(adv_audio)\n",
    "\n",
    "    adv_audio.requires_grad_()\n",
    "\n",
    "# Sauvegarde de l'audio apr√®s application du masquage psychoacoustique\n",
    "adversarial_audio = (adv_audio.detach().numpy() * 32768).astype(np.int16)\n",
    "wav.write(\"Test_tp_fasii_PGD_attack.wav\", 16000, adversarial_audio)\n",
    "\n",
    "# V√©rifier la transcription apr√®s attaque\n",
    "inputs = processor(adv_audio.unsqueeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(0)\n",
    "inputs.input_values = inputs.input_values.squeeze().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_adversarial = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"üö® Transcription after attack (psychoacoustic applied): {text_adversarial}\")\n",
    "print(f\"üîç Max absolute perturbation: {torch.max(torch.abs(adv_audio - audio_tensor)).item():.6f}\")\n",
    "print(\"‚úÖ Adversarial audio saved as 'Test_tp_fasii_PGD_attack.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original Transcription: THE SUN ALSO RISES POWERFUL INTENSE VISUALLY MAGNIFICENT THE SUN ALSO RISES IS THE NOVEL WHICH ESTABLISHED INATEMIGWAY AS A WRITER OF GENIUS\n",
      "üîç Perturbation Norm: 0.000000\n",
      "üîÑ Iteration 1\n",
      "üîç Loss: 8.279809951782227\n",
      "üîÑ Iteration 2\n",
      "üîç Loss: 7.828545570373535\n",
      "üîÑ Iteration 3\n",
      "üîç Loss: 7.6461591720581055\n",
      "üîÑ Iteration 4\n",
      "üîç Loss: 9.375314712524414\n",
      "üîÑ Iteration 5\n",
      "üîç Loss: 8.904016494750977\n",
      "üîÑ Iteration 6\n",
      "üîç Loss: 10.112112045288086\n",
      "üîÑ Iteration 7\n",
      "üîç Loss: 9.512578010559082\n",
      "üîÑ Iteration 8\n",
      "üîç Loss: 10.366658210754395\n",
      "üîÑ Iteration 9\n",
      "üîç Loss: 12.157768249511719\n",
      "üîÑ Iteration 10\n",
      "üîç Loss: 10.039602279663086\n",
      "üö® Transcription after attack (psychoacoustic applied): THE SUN ALSO ARISES POWERFUL AND TENSE VISUALLY MAGNIFICENT THE SUN ALTORIZES IS THE NOVEL WHICH ESTABLISHED NETHEMIC WIT AS A WRITER ARCHINIUS\n",
      "üîç Max absolute perturbation: 0.224000\n",
      "‚úÖ Adversarial audio saved as 'Tsg_long_version_PGD_attack.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "def convert_audio(input_path):\n",
    "    base, ext = os.path.splitext(input_path)\n",
    "    output_path = f\"{base}_converted.wav\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "def bark_scale(freq):\n",
    "    \"\"\"Convert frequency to the Bark scale.\"\"\"\n",
    "    return 13 * np.arctan(0.00076 * freq) + 3.5 * np.arctan((freq / 7500.0) ** 2)\n",
    "\n",
    "def apply_psychoacoustic_masking(waveform, sr=16000):\n",
    "    \"\"\"Appliquer un masquage psychoacoustique pour cacher les perturbations.\"\"\"\n",
    "    waveform_np = waveform.detach().cpu().numpy()  # D√©tacher pour utiliser librosa\n",
    "    stft = librosa.stft(waveform_np, n_fft=512, hop_length=128)\n",
    "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "\n",
    "    # Appliquer une pond√©ration bas√©e sur l'√©chelle de Bark\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)\n",
    "    bark_bins = bark_scale(freqs)\n",
    "    mask = np.exp(-bark_bins / np.max(bark_bins))  # Plus Bark est haut, moins de modification\n",
    "    masked_magnitude = magnitude * mask[:, np.newaxis]\n",
    "\n",
    "    # Reconstruction du signal\n",
    "    modified_stft = masked_magnitude * np.exp(1j * phase)\n",
    "    modified_waveform = librosa.istft(modified_stft, hop_length=128)\n",
    "\n",
    "    return torch.tensor(modified_waveform, dtype=torch.float32)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True)\n",
    "\n",
    "AUDIO_PATH = \"Tsg_long_version.wav\"\n",
    "AUDIO_PATH = convert_audio(AUDIO_PATH)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    raise ValueError(f\"Incorrect sample rate: {sample_rate} Hz\")\n",
    "\n",
    "waveform = waveform.mean(dim=0)\n",
    "waveform = waveform / waveform.abs().max()\n",
    "\n",
    "inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(1)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_original = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"‚úÖ Original Transcription: {text_original}\")\n",
    "\n",
    "audio_tensor = waveform.clone().detach().requires_grad_(True)\n",
    "\n",
    "epsilon = 0.03\n",
    "alpha = epsilon / 4\n",
    "num_iter = 10\n",
    "adv_audio = audio_tensor.clone().detach().requires_grad_(True)\n",
    "min_length = min(audio_tensor.shape[0], adv_audio.shape[0])\n",
    "audio_tensor = audio_tensor[:min_length]\n",
    "adv_audio = adv_audio[:min_length]\n",
    "\n",
    "perturbation_norm = torch.norm(adv_audio - audio_tensor).item()\n",
    "\n",
    "print(f\"üîç Perturbation Norm: {perturbation_norm:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "target = processor.tokenizer(text_original, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "input_lengths = torch.tensor([logits.shape[1]], dtype=torch.long, device=logits.device)\n",
    "target_lengths = torch.tensor([min(target.shape[0], logits.shape[1])], dtype=torch.long, device=logits.device)\n",
    "\n",
    "ctc_loss = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    print(f\"üîÑ Iteration {i+1}\")\n",
    "    adv_audio = adv_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "    if adv_audio.grad is not None:\n",
    "        adv_audio.grad.zero_()\n",
    "\n",
    "    logits = model(adv_audio.unsqueeze(0)).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    entropy_loss = -torch.sum(log_probs * torch.exp(log_probs))  # Increase uncertainty\n",
    "    alignment_loss = ctc_loss(log_probs.transpose(0, 1), target, input_lengths, target_lengths)\n",
    "    loss = alignment_loss + 0.15 * entropy_loss  # Weighted attack\n",
    "\n",
    "    loss.backward()\n",
    "    print(f\"üîç Loss: {loss.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        adaptive_alpha = alpha * torch.abs(adv_audio.grad) / torch.max(torch.abs(adv_audio.grad))\n",
    "        perturbation = adaptive_alpha * torch.sign(adv_audio.grad)\n",
    "        adv_audio = adv_audio + perturbation\n",
    "        perturbation = torch.clamp(adv_audio - audio_tensor, min=-epsilon, max=epsilon)\n",
    "        adv_audio = audio_tensor + perturbation\n",
    "        adv_audio = torch.clamp(adv_audio, min=-1, max=1)\n",
    "    \n",
    "    energy = torch.abs(torch.stft(adv_audio, n_fft=512, return_complex=True)).mean(dim=1)\n",
    "    high_energy_mask = (energy > torch.mean(energy)).float()\n",
    "\n",
    "    # Upsample high_energy_mask to match adv_audio's length\n",
    "    high_energy_mask = torch.nn.functional.interpolate(\n",
    "        high_energy_mask.unsqueeze(0).unsqueeze(0),  # Add batch & channel dims\n",
    "        size=adv_audio.shape[0],  # Resize to match waveform length\n",
    "        mode=\"linear\",  # Use linear interpolation for smooth scaling\n",
    "        align_corners=False\n",
    "    ).squeeze()  # Remove extra dims\n",
    "\n",
    "    energy_factor = 0.8  # Control how much energy is preserved (0.0 = mute, 1.0 = no change)\n",
    "    adaptive_mask = energy_factor + (1 - energy_factor) * high_energy_mask\n",
    "    adv_audio = adv_audio * adaptive_mask\n",
    "\n",
    "    adv_audio.requires_grad_()\n",
    "\n",
    "# Sauvegarde de l'audio apr√®s application du masquage psychoacoustique\n",
    "adversarial_audio = (adv_audio.detach().numpy() * 32768).astype(np.int16)\n",
    "wav.write(\"Tsg_long_version_PGD_attack.wav\", 16000, adversarial_audio)\n",
    "\n",
    "# V√©rifier la transcription apr√®s attaque\n",
    "inputs = processor(adv_audio.unsqueeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(0)\n",
    "inputs.input_values = inputs.input_values.squeeze().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_adversarial = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"üö® Transcription after attack (psychoacoustic applied): {text_adversarial}\")\n",
    "print(f\"üîç Max absolute perturbation: {torch.max(torch.abs(adv_audio - audio_tensor)).item():.6f}\")\n",
    "print(\"‚úÖ Adversarial audio saved as 'Tsg_long_version_PGD_attack.wav'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Original Transcription: VOICES AND ACTIVISTS HAVE BEEN ROUTINELY DENOUNCED MISREPRESENTED AND TARGETED BY MANY NATIONAL MEDIA OUTLETS THE RIGHT WING MEDIA HAS BEEN PARTICULARLY HOSTILE\n",
      "üîÑ Iteration 1\n",
      "üîç Loss: 13.96053695678711\n",
      "üîÑ Iteration 2\n",
      "üîç Loss: 15.860575675964355\n",
      "üîÑ Iteration 3\n",
      "üîç Loss: 20.12864112854004\n",
      "üîÑ Iteration 4\n",
      "üîç Loss: 27.799667358398438\n",
      "üîÑ Iteration 5\n",
      "üîç Loss: 30.671201705932617\n",
      "üîÑ Iteration 6\n",
      "üîç Loss: 31.34099578857422\n",
      "üîÑ Iteration 7\n",
      "üîç Loss: 25.946313858032227\n",
      "üîÑ Iteration 8\n",
      "üîç Loss: 26.023954391479492\n",
      "üîÑ Iteration 9\n",
      "üîç Loss: 35.7636604309082\n",
      "üîÑ Iteration 10\n",
      "üîç Loss: 38.63713836669922\n",
      "üö® Transcription apr√®s attaque combin√©e: VOICES AND ACTIVISTS HAVE BEEN LOUTINELY DENOUNCED MASSE REPRESENTED AND TARGOTED BY MANY NATIONAL MEDIA OUTLETS THE RIGHT WING MIGALE HAS BEEN PARTICULARLY AL STOT\n",
      "‚úÖ Adversarial audio saved as 'Test_tp_fasii_PGD_combined_attack.wav'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "def convert_audio(input_path):\n",
    "    base, ext = os.path.splitext(input_path)\n",
    "    output_path = f\"{base}_converted.wav\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "def bark_scale(freq):\n",
    "    \"\"\"Convert frequency to the Bark scale.\"\"\"\n",
    "    return 13 * np.arctan(0.00076 * freq) + 3.5 * np.arctan((freq / 7500.0) ** 2)\n",
    "\n",
    "def apply_psychoacoustic_masking(waveform, sr=16000):\n",
    "    \"\"\"Appliquer un masquage psychoacoustique pour cacher les perturbations.\"\"\"\n",
    "    waveform_np = waveform.detach().cpu().numpy()  # D√©tacher pour utiliser librosa\n",
    "    stft = librosa.stft(waveform_np, n_fft=512, hop_length=128)\n",
    "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "\n",
    "    # Appliquer une pond√©ration bas√©e sur l'√©chelle de Bark\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)\n",
    "    bark_bins = bark_scale(freqs)\n",
    "    mask = np.exp(-bark_bins / np.max(bark_bins))  # Plus Bark est haut, moins de modification\n",
    "    masked_magnitude = magnitude * mask[:, np.newaxis]\n",
    "\n",
    "    # Reconstruction du signal\n",
    "    modified_stft = masked_magnitude * np.exp(1j * phase)\n",
    "    modified_waveform = librosa.istft(modified_stft, hop_length=128)\n",
    "\n",
    "    return torch.tensor(modified_waveform, dtype=torch.float32)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True)\n",
    "\n",
    "AUDIO_PATH = \"Test_tp_fasii.wav\"\n",
    "AUDIO_PATH = convert_audio(AUDIO_PATH)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    raise ValueError(f\"Incorrect sample rate: {sample_rate} Hz\")\n",
    "\n",
    "waveform = waveform.mean(dim=0)\n",
    "waveform = waveform / waveform.abs().max()\n",
    "\n",
    "inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze(1)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_original = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"‚úÖ Original Transcription: {text_original}\")\n",
    "\n",
    "audio_tensor = waveform.clone().detach().requires_grad_(True)\n",
    "\n",
    "epsilon = 0.065\n",
    "alpha = epsilon / 3.5\n",
    "num_iter = 10\n",
    "adv_audio = audio_tensor.clone().detach().requires_grad_(True)\n",
    "min_length = min(audio_tensor.shape[0], adv_audio.shape[0])\n",
    "audio_tensor = audio_tensor[:min_length]\n",
    "adv_audio = adv_audio[:min_length]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target = processor.tokenizer(text_original, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "input_lengths = torch.tensor([logits.shape[1]], dtype=torch.long, device=logits.device)\n",
    "target_lengths = torch.tensor([min(target.shape[0], logits.shape[1])], dtype=torch.long, device=logits.device)\n",
    "\n",
    "ctc_loss = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    print(f\"üîÑ Iteration {i+1}\")\n",
    "    adv_audio = adv_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "    if adv_audio.grad is not None:\n",
    "        adv_audio.grad.zero_()\n",
    "\n",
    "    logits = model(adv_audio.unsqueeze(0)).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    entropy_loss = -torch.sum(log_probs * torch.exp(log_probs))  # Increase uncertainty\n",
    "    alignment_loss = ctc_loss(log_probs.transpose(0, 1), target, input_lengths, target_lengths)\n",
    "    loss = alignment_loss + 0.4 * entropy_loss  # Weighted attack\n",
    "\n",
    "    loss.backward()\n",
    "    print(f\"üîç Loss: {loss.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        adaptive_alpha = alpha * torch.abs(adv_audio.grad) / torch.max(torch.abs(adv_audio.grad))\n",
    "        perturbation = adaptive_alpha * torch.sign(adv_audio.grad)\n",
    "        adv_audio = adv_audio + perturbation\n",
    "        perturbation = torch.clamp(adv_audio - audio_tensor, min=-epsilon, max=epsilon)\n",
    "        adv_audio = audio_tensor + perturbation\n",
    "        adv_audio = torch.clamp(adv_audio, min=-1, max=1)\n",
    "    \n",
    "    # Appliquer le masquage psychoacoustique\n",
    "adv_audio = apply_psychoacoustic_masking(adv_audio)\n",
    "\n",
    "# Appliquer le masquage bas√© sur l'√©nergie\n",
    "energy = torch.abs(torch.stft(adv_audio, n_fft=512, return_complex=True)).mean(dim=1)\n",
    "high_energy_mask = (energy > torch.mean(energy)).float()\n",
    "\n",
    "# Upsample du masque pour qu'il corresponde √† la longueur du signal audio\n",
    "high_energy_mask = torch.nn.functional.interpolate(\n",
    "    high_energy_mask.unsqueeze(0).unsqueeze(0),  \n",
    "    size=adv_audio.shape[0],  \n",
    "    mode=\"linear\",\n",
    "    align_corners=False\n",
    ").squeeze()  \n",
    "\n",
    "energy_factor = 0.2  # Contr√¥le du niveau d'att√©nuation des perturbations\n",
    "adaptive_mask = energy_factor + (1 - energy_factor) * high_energy_mask\n",
    "adv_audio = adv_audio * adaptive_mask\n",
    "\n",
    "# Sauvegarde du signal perturb√©\n",
    "adversarial_audio = (adv_audio.detach().numpy() * 32768).astype(np.int16)\n",
    "wav.write(\"Test_tp_fasii_PGD_combined_attack.wav\", 16000, adversarial_audio)\n",
    "\n",
    "# V√©rifier la transcription apr√®s attaque combin√©e\n",
    "inputs = processor(adv_audio.unsqueeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_values = inputs.input_values.squeeze().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "text_adversarial = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(f\"üö® Transcription apr√®s attaque combin√©e: {text_adversarial}\")\n",
    "print(\"‚úÖ Adversarial audio saved as 'Test_tp_fasii_PGD_combined_attack.wav'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
